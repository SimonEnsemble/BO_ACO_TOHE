\documentclass[fleqn,10pt,lineno]{wlpeerj}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage{xspace}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{tcolorbox} 
\tcbuselibrary{breakable}
\newtcolorbox[auto counter
%,number within=section
]{mybox}[2][]{
title=Box~\thetcbcounter: #2,#1,
colback=white,
colframe=gray,
fonttitle=\bfseries,
parbox=false
}
\usepackage{enumitem}


\title{Bi-objective trail-planning for a robot team orienteering in a hazardous environment}
\author[1]{Cory M. Simon$^*$}
\author[2]{Jeffrey Richley}
\author[2]{Lucas Overbey$^\ddagger$}
\author[2]{Darleen Perez-Lavin$^\dagger$}
\affil[1]{School of Chemical, Biological, and Environmental Engineering. Oregon State University. Corvallis, OR. USA.}
\affil[2]{Naval Information Warfare Center Atlantic. Charleston, SC. USA.}
 \corrauthor[1]{Cory M. Simon}{cory.simon@oregonstate.edu}


%\flushbottom
% \maketitle
%\thispagestyle{empty}


%\affil[*]{}
% \date{}							% Activate to display a given date or no date

\begin{abstract}
Teams of mobile [aerial, ground, or aquatic] robots have applications in resource delivery, patrolling, information-gathering, agriculture, forest fire fighting, chemical plume source localization and mapping, and search-and-rescue. Robot teams traversing hazardous environments---with e.g.\ rough terrain or seas, strong winds, or adversaries capable of attacking or capturing robots---should plan and coordinate their trails in consideration of risks of disablement, destruction, or capture. Specifically, the robots should take the safest trails, coordinate their trails to cooperatively achieve the team-level objective with robustness to robot failures, and balance the reward from visiting locations against risks of robot losses. \\

Herein, we consider bi-objective trail-planning for a mobile team of robots orienteering in a hazardous environment. The hazardous environment is abstracted as a directed graph whose arcs, when traversed by a robot, present known probabilities of survival. Each node of the graph offers a reward to the team if visited by a robot (which e.g.\ delivers a good to or images the node). We wish to search for the Pareto-optimal robot-team trail plans that maximize two [conflicting] team objectives: the expected (i) team reward and (ii) number of robots that survive the mission. A human decision-maker can then select trail plans that balance, according to their values, reward and robot survival. We implement ant colony optimization, guided by heuristics, to search for the Pareto-optimal set of robot team trail plans. As a case study, we illustrate with an information-gathering mission in an art museum.
% nuclear power plant from a Defense Advanced Research Projects Agency (DARPA) robots challenge.

\end{abstract}

\begin{document}

\flushbottom
\maketitle
\thispagestyle{empty}



\clearpage


\section{Introduction}
\subsection{Applications of a team of mobile robots}
Mobile [aerial \cite{leutenegger2016flying}, ground \cite{chung2016wheeled}, or aquatic \cite{choi2016underwater}] robots equipped with sensors, actuators, and/or cargo have applications in agriculture 
% (e.g.\ planting and harvesting crops, spraying pesticide, monitoring crop health, destroying weeds) 
\cite{santos2020path,bawden2017robot,mcallister2018multi}, 
commerce % (e.g.\ order fulfillment in warehouses)
 \cite{wurman2008coordinating}, 
the delivery of goods \cite{coelho2014thirty}, 
search-and-rescue \cite{queralta2020collaborative,rouvcek2020darpa}, 
chemical, biological, radiological, or nuclear incident response %(e.g.\ safely localizing the source(s) and mapping the distribution of the hazard)
 \cite{murphy2012projected,hutchinson2019unmanned}, 
 environmental monitoring \cite{dunbabin2012robots,hernandez2012mobile,yuan2020maritime}, 
 process monitoring in industrial chemical plants \cite{soldan2014towards,francis2022gas}, 
 forest fire monitoring and fighting \cite{merino2012unmanned}, 
 wildlife conservation \cite{kamminga2018poaching},
 patrolling \cite{basilico2022recent},
 target tracking \cite{robin2016multi}, and 
 military surveillance and reconnaissance. 
 
Algorithms for both low-level, tactical and high-level, strategic planning of the paths/routes of mobile robots in static or dynamic environments \cite{lavalle2006planning,liu2023path,ugwoke2025simulation,siegwart2011introduction} are vital for autonomy and pivotal for efficiency, safety, and good performance on the objectives.

The classic, low-level path-planning problem for a robot \cite{lavalle2006planning,agrawal2022classical} involves finding the least-cost (e.g., shortest), collision-free path from a starting location to a destination in a static, known environment containing obstacles.
% modeled as a grid, where obstacles render some grid cells inaccessible.
By discretizing the environment with a cell decomposition method \cite{latombe2012robot}, we can model the environment as a graph, where nodes represent locations and edges represent traversable connections between locations. Then, a variety of graph search (e.g., Dijkstra's algorithm \cite{candra2020dijkstra}), random sampling (e.g., rapidly exploring random trees \cite{lavalle1998rapidly}), heuristic (e.g., A-star \cite{candra2020dijkstra}), meta-heuristic (e.g., ant colony optimization \cite{brand2010ant}), and reinforcement learning \cite{singh2023review} algorithms can be adopted for finding a low-cost, collision-free path in this graph. 
The selection of a path-planning algorithm involves tradeoffs in solution quality, computational cost, memory usage, parallelizability, and scalability \cite{ugwoke2025simulation}.
In multi-robot path-planning \cite{wagner2011m}, each robot has unique start and end locations; robot-robot collisions must be avoided as well as robot-obstacle collisions; and the configuration space is much larger.

For many applications, we wish for a team of multiple mobile robots to coordinate their routes in an environment to cooperatively achieve a shared, team-level objective \cite{parker1995design,parker2007distributed}.
Compared to a single robot, a robot team can increase spatial coverage, enhance performance on the high-level objectives at hand, achieve the objectives more quickly, and make achievement of the objectives robust to the failure of some robots \cite{schranz2020swarm,brambilla2013swarm}.
Multi-robot, coordinative route-planning involves high-level, strategic decision-making; fine-grained details like obstacle avoidance and the exact paths in Euclidean space are delegated to lower-level decision-making systems. 
Typically, a multi-robot, coordinative routing problem is framed as a combinatorial optimization problem.

%, framed as a combinatorial optimization problem,
% Multi-robot, coordinative routing problems are framed as combinatorial optimization problems.
A prominent example of a multi-robot, coordinative route-planning is the team orienteering \cite{golden1987orienteering} problem (TOP) \cite{chao1996team,gunawan2016orienteering,vansteenwegen2011orienteering}.
In the TOP, a team of robots are mobile in an environment modeled as a graph, and each node gives a reward to the team if visited by a robot.
The TOP is to plan the routes of the robots, from a source to destination node, to gather the most rewards as a team under a travel budget for each robot. 
Loosely, the TOP problem combines aspects of the classic knapsack problem (selecting the nodes from which to collect rewards, under the travel budget) and the traveling salesman problem (finding the shortest path that visits these nodes) \cite{vansteenwegen2011orienteering}.
The TOP can be formulated as an integer program.
Approaches to obtain a solution to the TOP include exact (e.g., a branch-and-price scheme \cite{boussier2007exact}), heuristic \cite{chao1996team}, and meta-heuristic (e.g., ant colony optimization \cite{ke2008ants}) algorithms \cite{vansteenwegen2011orienteering}. Again, algorithm selection involves tradeoffs in solution quality and computational efficiency. 

\subsection{Teams of mobile robots orienteering in hazardous environments} 
In some applications, robots move in a hazardous environment \cite{trevelyan2016robotics} and incur risks of failure, destruction, disablement, and/or capture. 
These hazards could originate from dangerous terrain, rough seas, strong winds, heat, radiation, corrosive chemicals, or mines---or, an adversary with the capability to attack and destroy, disable, or capture robots \cite{agmon2017robotic}. 

Robots traversing a hazardous environment should plan and coordinate their routes in consideration of risks of failure.
First, each robot should take the safest route to visit its destination(s). 
Second, the robots should collaborate when planning their routes to make achievement of the team objective resilient to robot failures \cite{zhou2021multi}. 
A \emph{resilient} team of robots \cite{prorok2021beyond}
(i) anticipates failures and makes risk-aware route plans that endow the team with \emph{robustness}---the ability to withstand failures with minimal concession of the objective---e.g., by redundantly assigning multiple robots to a task requiring only one robot,
and/or
(ii) adapts their route plans during the mission in response to realized robot failures to recoup the otherwise anticipated loss in the objective. 
Third, the route plans must balance the rewards gained from visiting different locations against the risks incurred by the robots to reach those locations.

A few route-planning problems for robots orienteering in hazardous environments, abstracted as graphs, have been framed and solved \cite{zhou2021multi}. 
In the Team Surviving Orienteers problem (TSOP) \cite{jorgensen2018team,jorgensen2017matroid,jorgensen2024matroid}, each node of the graph offers a reward to the team when visited by a robot, and a robot incurs some probability of destruction with each edge-traversal.
The objective in the TSOP is to plan the paths of the robots, from a source to destination node, to maximize the expected team reward. As a constraint, each robot must survive the mission with a probability above a tolerable threshold. 
In the \emph{offline} TSOP, the paths of the robots are set at the beginning of the mission, then followed without adaptation; in the \emph{online} setting, the paths are updated during the mission in response to realized robot failures.
%In the extended Matroid TSO problem \cite{jorgensen2017matroid,jorgensen2024matroid}, we seek to maximize the weighted
%expected number of nodes visited by one or more robots.
Relatedly, the Foraging Route with the Maximum Expected Utility problem \cite{di2022foraging} is to plan the foraging route of a robot collecting rewards in a hazardous environment, but the rewards are lost if the robot is destroyed before returning to the source node to deposit the goods it collected.
In the Robust Multiple-path Orienteering Problem (RMOP) \cite{shi2023robust}, similarly, each node gives a reward to the team only if a robot visits it \emph{and} survives the mission to deposit the reward. The paths of the $K$ robots, subject to travel budgets, are planned to maximize the team reward under the worst-case attacks of $\alpha<K$ of the robots by an adversary. 
The offline version of the RMOP constitutes a two-stage, sequential game with perfect information: (1) the robot team chooses a set of paths then (2) the adversary, knowing these paths, chooses the subset of robots to attack and destroy. 
The optimal path plans for the robots must trade (i) redundancy in the nodes visited, to give robustness against attacks, and (ii) coverage of many nodes, to collect many rewards.
In the hazardous orienteering problem (HOP) \cite{santini2023hazardous,montemanni2025exact}, robots incur a risk of destruction while carrying valuable parcels picked up at some nodes, and the reward is received only if the robot returns safely to the depot node.
Notably, Subramanian (1997) was perhaps the first to model stochastic hazards encountered by a robot taking a path on a graph \cite{sherali1997low}.
Other work involving robots traversing hazardous environments includes 
maximizing coverage of an area containing threats to robots \cite{korngut2023multi,yehoshua2016robotic}, 
handling adversarial attacks on the sensors of the robots \cite{liu2021distributed,zhou2022distributed,mayya2022adaptive,zhou2018resilient}, 
gathering information in an environment with unknown hazards \cite{schwager2017multi},
finding the optimal formation for a robot team \cite{shapira2015path},
and 
multi-robot patrolling under adversarial attacks \cite{huang2019survey}.
Though not a robot routing problem, the time-bomb knapsack problem \cite{monaci2022exact} is relevant to robots carrying hazardous cargo or cargo that invites attacks: maximize the expected profit of items placed in a knapsack under a capacity when each item has a probability of exploding and causing the profit to be lost.

\subsection{Our contribution}
Herein, our contribution is:
(1) framing and intuiting a bi-objective variant of the offline TSOP \cite{jorgensen2018team,jorgensen2017matroid,jorgensen2024matroid}, the bi-objective team orienteering in hazardous environments (BOTOHE) problem, then 
(2) specifying, implementing, and benchmarking a bi-objective ant colony optimization algorithm \cite{iredi2001bi}, guided by BOTOHE-specific heuristics, to search for the Pareto-optimal set of robot-team trail plans, then
(3) solving and analyzing BOTOHE problem instances on synthetic and real-world graphs for insights.

In the BOTOHE problem, 
%Then, we use a bi-objective ant colony optimization algorithm, guided by heuristics, to search for the Pareto-optimal robot-team trail plans.
%For illustration, we solve a BOTOHE instance for an information-gathering mission in an art museum. % nuclear power plant in a Defense Advanced Research Projects Agency (DARPA) robots challenge.
% \paragraph{The bi-objective team orienteering in hazardous environments problem (BOTOHE).} 
a team of robots are mobile in a hazardous environment, modeled as a directed graph where each arc presents a known probability of destruction to a robot that traverses it.
Each node of the graph gives a reward to the team if visited by a robot.
The robots begin at a source node.
The BOTOHE problem is to plan the closed trails of the robots to maximize two team-level objectives: the expected
(1) rewards accumulated by the team and
(2) number of robots that survive the mission. 
See Fig.~\ref{fig:overview}.
(We focus on the offline setting, corresponding with a lack of communication with/between robots after the mission executes.)


Three interesting features of the BOTOHE are: 
(1) for the survival objective, robots a) risk-aversely avoid visiting dangerous subgraphs despite rewards offered by nodes therein and b) take the safest closed trails to visit the nodes assigned to them;
(2) for the reward objective, the robots a) daringly visit dangerous subgraphs to attempt collection of the rewards offered by nodes therein, b) visit the lower-risk and higher-reward subgraphs earlier in their trails, and c) build node-visit redundancy into their trail plans (i.e., multiple robots plan to visit the same node despite zero marginal reward for a second, third, and so-on visit) to make the team-reward robust to the loss of robots during the mission; and
(3) comparing (1a) and (2a), the two objectives are inherently conflicting, as the robots must risk their survival while taking their trails to visit nodes and collect rewards.%\footnote{Two extremes: (1) to maximize survival, the robots never leave the base node no matter the rewards to be gained and (2) to maximize reward, each robot traverses the entire graph no matter the risks involved.}.

To handle the conflict between the reward and survival objectives in the BOTOHE, we search for the Pareto-optimal set \cite{pardalos2017non,branke2008multiobjective} of robot-team trail plans. By definition, a Pareto-optimal robot-team trail plan cannot be altered to give a higher expected reward without lowering the expected number of robots that survive---and vice versa. The Pareto front is the set of expected (reward, survivals) objective vectors corresponding with the Pareto-optimal set of robot-team trail plans.
Fig.~\ref{fig:pareto_optimal} illustrates.
As opposed to aggregating the two objectives into a single, scalar objective, a Pareto front (i) reveals the tradeoffs between expected rewards and robot survivals and (ii) allows us to delay placement of quantitative preferences on the two objectives.
Later, a human decision-maker presented with the Pareto-front can examine the tradeoffs then make an informed selection of a Pareto-optimal robot-team trail plan that suites their preferences/values at the moment.

To search for the Pareto-optimal set of robot-team trail plans in the BOTOHE, we specify, implement, and benchmark a bi-objective ant colony optimization (ACO) algorithm \cite{iredi2001bi}, guided by heuristics.
We adopt the ACO meta-heuristic \cite{dorigo2006ant,bonabeau1999swarm,blum2005ant} for the BOTOHE for several reasons.
Generally, ACO, a swarm intelligence method, excels at searching for [near-]optimal trails on graphs.
In ACO, a simulated colony of decentralized ants constructs trails stochastically over iterations while 
(1) collectively learning from feedback (i.e., computed objective values associated with trails they construct) by laying pheromone on their constructed trails in proportion to their quality (i.e., objective value) and
(2) during trail construction, balancing 
(a) exploitation of the colony's memory of good trails via biasing towards taking arcs with high pheromone and 
(b) exploration of new trails via stochasticity and pheromone evaporation.
Advantageously, ACO allows us to incorporate BOTHE-specific heuristics to bias ants' trail construction and accelerate convergence.
ACO handles our bi-objective problem by (1) assigning ants to specialize to different regions of the Pareto front and (2) maintaining two species of pheromone---one for each objective---on the arcs \cite{iredi2001bi}.
Finally, ACO is computationally efficient (compared to integer program solvers \cite{pascariu2021train}), straightforward to implement, embarrassingly parallelizable (allocate one ant per thread), and likely extendable to efficiently handle dynamic or online BOTHE problem variants via using the stored pheromone trail as a warm start \cite{montemanni2005ant}.

For illustration, we solve and analyze a series of three BOTOHE problem instances for an information-gathering mission on:
(1) a synthetic graph, sampled from a stochastic block model, with two communities;
(2) real-world graphs modeling (2a) the San Diego art museum and (2b) the [unfinished] Satsop Nuclear Power Plant in Washington used for the  Defense Advanced Research Projects Agency (DARPA) Subterranean Robotics Challenge \cite{ackerman2022robots,orekhov2022darpa}.
We explore the Pareto-optimal set of robot-team trail plans for the art museum to gain insights.
For each problem instance, we quantify the contribution of (a) the greedy heuristics and (b) the pheromone to the search efficiency of ACO through two ablation studies. We also benchmark bi-objective ACO against (a) random search and (b) bi-objective simulated annealing.
% from the Defense Advanced Research Projects Agency (DARPA) Subterranean (SubT) robotics challenge \cite{chung2023into} (Alpha course of the Urban Circuit Network \cite{github_darpa_subt}).

\begin{figure}[h!]
    \centering
     \begin{subfigure}[b]{0.62\textwidth}
    	\includegraphics[width=\textwidth]{overview_2.pdf}
	\caption{} \label{fig:overview}
    \end{subfigure}
    \begin{subfigure}[b]{0.66\textwidth}
    	\includegraphics[width=\textwidth]{toy_pareto_front2.pdf}
	\caption{} \label{fig:pareto_optimal}
    \end{subfigure}
    \caption{
      The bi-objective team orienteering in hazardous environments (BOTHE) problem.
      (a) A team of robots are mobile on a directed graph whose 
      nodes offer a reward to the team if visited by a robot and 
      arcs present a probability of destruction to robots that traverse them (tornado: 1/10 probability of destruction). The task is to plan the trails of the robots to maximize the expected reward collected and the expected number of robots that survive.
      (b) Pareto-optimal and -dominated robot-team trail plans scattered in objective space, with two Pareto-optimal plans and one Pareto-dominated plan shown.}
\end{figure}



\section{The bi-objective team orienteering in hazardous environments (BOTHE) problem}
%In the risky team orienteering problem (RTOP), our task is to plan the trails of a team of mobile robots on a directed graph whose (i) nodes offer rewards to the team depending on the number of robots that visit them and (ii) edges, when traversed by a robot, impose a risk of robot failure/destruction.
%The trails are set at the beginning of the mission, then followed by the robots without updates during the mission---an offline setting. 
%For the bi-objective RTOP (BO-RTOP), we wish to find the set of Pareto-optimal trail plans for the robot team that maximize the expected (i) rewards collected by the team and (ii) the number of robots that survive the mission.

%A team of mobile robots must plan closed trails to follow on a directed graph, containing hazards, to collect rewards from nodes. The two conflicting objectives are to maximize the expected team-reward and number of robots that survive. We seek the Pareto-optimal set of robot-team trail plans.


\subsection{Problem setup}
Here, we frame the Bi-Objective robot-Team Orienteering in Hazardous Environments (BOTOHE) problem. 

A homogenous team of $K$ robots are mobile in an environment modeled as a directed graph $G=(\mathcal{V}, \mathcal{E})$. Each node $v \in \mathcal{V}$ represents a location in the environment (e.g., a room in a building). Each arc $(v, v^\prime) \in\mathcal{E}$, an ordered pair of distinct nodes, represents a one-way, direct spatial connection (e.g.\ a door- or hall-way) to travel from node $v$ to node $v^\prime$. 
\emph{Mobility} of the robots implies they may walk on the graph $G$, i.e. sequentially hop from a node $v$ to another node $v^\prime$ via traversing arc $(v, v^\prime)\in\mathcal{E}$.
All $K$ robots begin at a base node $v_b \in \mathcal{V}$. 
% TODO revisit
% We include one self-loop $(v_b, v_b) \in \mathcal{E}$ to allow the possibility for a robot to never leave the base node.
We assume $G$ is strongly connected so that each node is reachable.
% ---but, not necessarily complete. 

Owing to unpredictable and/or uncertain hazards in the environment, a robot incurs a probability of destruction of $1 - \omega(v, v^\prime)$ when, beginning at node $v$, it attempts to traverse arc $(v, v^\prime) \in \mathcal{E}$ to visit node $v^\prime$.
% following its trail $\rho$. 
Each outcome (survival or destruction) is an independent event. 
The arc survival probability map $\omega: \mathcal{E} \rightarrow (0, 1]$ is known, static over the course of the mission, and not necessarily symmetric (owing to e.g., [directional] air/water currents or sunlight or an adversary with limited attack range nearer one node than another).

%\vspace{-\baselineskip}
%\subparagraph{Interpretation.} 
%Each node $v\in \mathcal{V}$ represents a distinct location in the environment (e.g., a room in a building or a house in a neighborhood).
%Each arc $(v, v^\prime) \in \mathcal{E}$ represents a direct spatial connection (e.g., a doorway or a road) for traveling from location $v$ to location $v^\prime$.


% the best (e.g., shortest or safest) path (in Euclidean space) for a mobile robot to take from the location represented by node $v$ to the location represented by $v^\prime$.
% Note, we do not assume the graph is complete\footnote{i.e., not every pair of distinct nodes $\{v, v^\prime\}$ is joined by two edges $(v, v^\prime)$ and $(v^\prime, v)$. 
% e.g., consider a building with node $v$ representing a room on the first floor, node $v^\prime$ a room on the second floor, and node $u$ as the staircase between the first and second floors. Traveling from node $v$ to node $v^\prime$ necessitates passing through node $u$ first.}.

Each node $v\in \mathcal{V}$ of the graph $G$ offers a reward $r(v) \in  \mathbb{R}_{\geq 0}$ to the robot team if visited by one or more robots over the course of the mission.
The reward $r(v)$ quantifies the utility gained by the team when a robot e.g.\ delivers a resource to node $v$, takes an image of node $v$ and transmits it back to the command center, or actuates some process (e.g.\ turns a valve) at node $v$. 
The total reward collected by the team is additive among nodes of the graph. % The first objective of the BOTOHE is to maximize the team reward.
Note, 
(1) even if a robot is destroyed after leaving a node, the harvested reward from that node is still accumulated by the team; and
(2) multiple visits to a node by the same or distinct robot(s) do not give marginal reward over a single visit. % I.e.\ the reward from a node is irrevocably collected by the team [only] by the first robot to visit it.
%; and
%(3) the reward offered by a node could be negative, i.e. there could be a penalty to visit a node instead of an incentive.



%\paragraph{The probabilistic model of robot destruction during trail-following.} 

% The second objective of the BOTOHE is to maximize the number of robots that s.

%Thus, from the function  that assigns robot survival probabilities to each arc of the graph $G$, we can compute the survival probabilities of the $K$ robots following any given set of trails.
% plans $\{\rho_1, ..., \rho_K\}$.% and (2) the expected utility of the rewards harvested by the robots along their paths, which we write next. 



%\vspace{-\baselineskip}
%\subparagraph{Interpretation.} The hazards in the environment could originate from obstacles the robot could crash into, rough terrain or seas, severe weather, mines, corrosive chemicals, or adversaries capable of attacking the robots at the arcs and/or nodes.
%The stochasticity of the survival of a robot traversing an arc originates from e.g. (i) the unpredictability of an aerial robot crashing into an obstacle, a ground robot falling over rocks, or a surface aquatic robot succumbing to ocean waves, or (ii) adversaries with (a) an imperfect capability to detect and attack robots and/or (b) uncertain presence in the environment.

%\vspace{-\baselineskip}
%\subparagraph{Possible asymmetry.} We do not assume $\omega$ is symmetric, i.e., that $\omega(v, v^\prime) = \omega(v^\prime, v)$. The traversal from node $v$ to $v^\prime$ may be more dangerous than from $v^\prime$ to $v$ owing to e.g., (i) strong air or water currents in the direction $v^\prime$ to $v$ (asymmetric arc traversal) or (ii) an adversary with attack capability at node $v^\prime$ but not at $v$ (asymmetric dangers at nodes). %Even if edge traversal risks are symmetric, the action of visiting a node be risky, and node $v$ may be more or less dangerous than node $v^\prime$, breaking symmetry. 

% \paragraph{The robot-team trail plan.}
To collect rewards in this hazardous environment, the team of robots must plan a set\footnote{Since the robot team is homogenous, we consider the team trail plan as permutation-invariant and thus treat it as a set not a list.} of closed, directed trails $\mathcal{P}:=\{\rho_1, ..., \rho_K\}$ on the graph $G$ to follow.
% obot $k$ on the team plans to execute/follow a closed, directed trail $\rho_k$ on the graph $G$.  
% The set\footnote{Since the robot team is homogenous, we consider the optimal trail plans for the robots as permutation-invariant and thus track it as a set not a list.} of closed, directed trails $\mathcal{P}:=\{\rho_1, ..., \rho_K\}$ the robot-team plans to follow constitute the \emph{robot team trail plan} for the mission. 
A \emph{directed trail} \cite{clark1991first,graphtheory2} is an ordered list of nodes $\rho = (\rho[0], \rho[1], ..., \rho[\lvert \rho \rvert])$ where
(i) $\rho[i] \in \mathcal{V}$ is the $i$th node in the trail,  
(ii) for each node in the trail, an arc exists from it to the subsequent node, i.e., $(\rho[i-1], \rho[i])\in\mathcal{E}$ for $1 \leq i  \leq \lvert \rho \rvert$,
(iii) $\lvert \rho \rvert$ is the number of arcs traversed in the trail,
and
(iv) the arcs traversed in the trail are unique, i.e. each arc in the multiset $\{(\rho[i-1], \rho[i])\}_{i=1}^{\lvert \rho \rvert}$ has a multiplicity of one.
Note, unlike a path, the nodes in a trail are not necessarily distinct \cite{wilson1979introduction}.
A \emph{closed} trail begins and ends at the same node, i.e. $\rho [0]=\rho[\lvert \rho \rvert]$, which, here, $=v_b$.
Each trail $\rho_k$ belonging to the robot-team trail plan $\mathcal{P}$ constitutes a \emph{plan} because robot $k$ may be destroyed in the process of following $\rho_k$ and thus not \emph{actually} visit all nodes in $\rho_k$. A robot \emph{survives} the mission if it visits all nodes in its planned trail and returns to the base node. I.e., robot $k$ survives the mission if and only if it survived each edge traversal in its trail plan $\rho_k$.
%\vspace{-\baselineskip}
%\subparagraph{The static/offline setting.} 
%The robot-team trail plan $\mathcal{P}$ are set at the beginning of the mission, then followed by the robots without adaptation or updates during the mission in response to observing robot failure(s).
%i.e., robots cannot communicate their survival status to the command center during the mission and/or the command center cannot send updated instructions to the robots after the mission executes.

We wish to design the robot-team trail plan $\mathcal{P}$ to maximize two objectives: 
(1) the expected team \underline{r}eward, $\mathbb{E}[R]$, and (2) the expected number of robots that \underline{s}urvive the mission, $\mathbb{E}[S]$. Both $R$ and $S$ (i) are random variables owing to the stochasticity of robot survival while trail-following and (ii) depend on the robot-team trail plan $\mathcal{P}$ owing to different rewards among nodes and different dangers among arcs.
That is, the BOTHE problem is:
\begin{equation}
	\max_{\mathcal{P}=\{\rho_1, ..., \rho_K\}} \left( \mathbb{E}[R(\mathcal{P})], \mathbb{E}[S(\mathcal{P})] \right)
	\label{eq:the_two_objs}
\end{equation}
when given the directed graph $G$ as a spatial abstraction of the environment, 
the homogenous team of $K$ mobile robots starting at the base node $v_b$,
the node reward map $r: \mathcal{V} \rightarrow \mathbb{R}_{\geq 0}$, and the arc survival probability map $\omega : \mathcal{E} \rightarrow (0, 1]$.
% the BOTHE problem is to determine the optimal robot-team [closed] trail plan $\mathcal{P}$ that maximizes two-objectives, (1) the expected team-reward and (2) the expected number of robots that survive the mission:

Because the bi-objective optimization problem in eqn.~\ref{eq:the_two_objs} presents a conflict between designing the robot-team trail plan to maximize the expected reward \emph{and} the expected number of surviving robots, we seek the \emph{Pareto-optimal set} \cite{pardalos2017non,branke2008multiobjective} of team-robot trail plans. 
The conflict is: 
(1) to maximize survival, a risk-averse robot team would not visit a dangerous region even if large rewards were contained therein, sacrificing reward for survival; 
(2) to maximize reward, a daring robot team would visit a dangerous region even if only small rewards were contained therein, sacrificing survival for reward. 
Hence, a utopian robot-team trail plan that simultaneously maximizes \emph{both} reward and survival objectives is unlikely to exist; the ultimate team trail plan selected for the mission must strike some tradeoff between reward and survival.
By definition, a \emph{Pareto-optimal} \cite{pardalos2017non,branke2008multiobjective} robot-team trail plan $\mathcal{P}^*$ cannot be altered to increase the survival objective $\mathbb{E}[S(\mathcal{P}^*)]$ without compromising (decreasing) the reward objective $\mathbb{E}[R(\mathcal{P}^*)]$---and vice versa. See Fig.~\ref{fig:pareto_optimal} and the formal definition below.
The Pareto-optimal set of team trail plans is valuable for presenting a \emph{portfolio} of team trail plans to a human decision-maker, who may then examine the possible tradeoffs then ultimately invoke their values---i.e., make a team-reward vs.\  robot-survival tradeoff---by selecting a reasonable team trail plan from the Pareto set.



% TODO say later somehwere: For the reward objective, the daring robots \emph{all} plan to enter dangerous regions---regardless of the potential reward; such node-visitation redundancy endows the team reward with robustness to robot failures. 


%\vspace{-\baselineskip}
%\subparagraph{Objective \#1: the expected reward collected by the robots.}
%Let $T_v(\mathcal{P}) $ be the number of robots on the team with trail plans $\mathcal{P}$ that ultimately visit node $v$.
%Because of the stochasticity of robot survival while trail-following, and thus of which nodes in the trail plan each robots ultimately visits, $T_v$ is a random variable.
%Then, the total team \underline{r}ewards collected by the robot-team following trail plans $\mathcal{P}$ is also a random variable:
%\begin{equation}
%R(\mathcal{P}) = \sum_{v\in\mathcal{V}} r_v\left ( T_v(\mathcal{P}) \right)
%\end{equation}
%that sums the rewards received from each node of the graph $G$.
%
%\vspace{-\baselineskip}
%\subparagraph{Objective \#2: the expected number of robots that survive the mission.}
%Let $S(\mathcal{P})$ be the number of robots, on a robot team with trail plans $\mathcal{P}$, that ultimately \underline{s}urvive the mission. Because of the stochasticity of robot survival while trail-following, $S$ is a random variable. 
%
%We wish to devise robot-team trail plans $\mathcal{P}$ to maximize both (1) the expected team reward, $\mathbb{E}[R(\mathcal{P})]]$, and (2) the expected number of robots that survive the mission, $\mathbb{E}[S(\mathcal{P})] \in (0, K]$. We derive formulas for the values of the two objectives in terms of the robot-team trail plans later (see eqns.~\ref{eq:formula_obj1} and \ref{eq:formula_obj2}). 



%, and (iv) do not assume the team reward viewed as a function over the set of visited nodes is monotone because we allow for negative rewards.
% {\color{red} k then make this happen in example, and how then does $\eta_r$ work if it can be negative? }



%Given the conflict between the two objectives, 

% A human decision-maker must invoke their values to make this tradeoff.% and determine if robots should plan to enter dangerous regions to attempt collection of large rewards there. 


\paragraph{Formal definition of Pareto-optimality and Pareto-front.}
Plan $\mathcal{P}^*$ belongs to the Pareto-optimal set of plans if and only if no other plan $\mathcal{P}^\prime$ \emph{Pareto-dominates} it. By definition, a plan $\mathcal{P}^\prime$ Pareto-dominates plan $\mathcal{P}^*$ if and only if both:
\begin{align}
	\left (\mathbb{E}[R(\mathcal{P}^\prime)] \geq \mathbb{E}[R(\mathcal{P}^*)]  \right) & \wedge \left( \mathbb{E}[S(\mathcal{P}^\prime)] \geq \mathbb{E}[S(\mathcal{P}^*)] \right) \\
	\left( \mathbb{E}[R(\mathcal{P}^\prime)] > \mathbb{E}[R(\mathcal{P}^*)] \right) & \vee \left( \mathbb{E}[S(\mathcal{P}^\prime)] > \mathbb{E}[S(\mathcal{P}^*)] \right).
\end{align}
So, a Pareto-dominating plan $\mathcal{P}^\prime$ is not worse than a Pareto-dominated plan $\mathcal{P}^*$ for reward nor for survivability and is better for one or both of them.
If a plan $\mathcal{P}^\prime$ were to Pareto-dominate another plan $\mathcal{P}^*$, one would objectively never choose plan $\mathcal{P}^*$ over plan $\mathcal{P}^\prime$, regardless of the values they place on the two objectives. The \emph{Pareto front} is the set of objective vectors $\{(\mathbb{E}[R(\mathcal{P}^*)], \mathbb{E}[S(\mathcal{P}^*)])\}$ associated with the Pareto-optimal set of team trail plans $\{\mathcal{P}^*\}$.  

\paragraph{Comparison with TSOP.}
The BOTHE problem formulation follows the offline TSOP \cite{jorgensen2018team} with three modifications: we 
(i) omit the constraints that each robot survives above a threshold probability (e.g., we accept if one [un-crewed] robot bears much more risk of destruction than another during the mission),
(ii) allow robots to follow trails instead of restricting to paths, as paths prevent robots from visiting a given node more than once and thus from e.g., harvesting reward from a leaf node having one in-degree and one out-degree involving the same node, 
and
(iii) specify two objectives instead of one and seek the Pareto-optimal set of team trail plans to examine trade-offs between reward and survival before selecting a team trail plan.

\subsection{Probability distributions and expectations of $R(\mathcal{P})$ and $S(\mathcal{P})$}
We now write formulas for the two objectives---the expectations of the team reward $R(\mathcal{P})$ and of number of robots that survive the mission $S(\mathcal{P})$---as a function of the robot-team trail plan $\mathcal{P}$. 
These formulas follow from the directed graph $G$, arc survival probability map $\omega$, and node reward map $r$. Fig.~\ref{fig:notation} illustrates our notation.
% Given a proposed robot-team trail plan $\mathcal{P}$, we can calculate the two objectives $\mathbb{E}[R(\mathcal{P})]$ and $\mathbb{E}[S(\mathcal{P})]$ via eqns.~\ref{eq:formula_obj1} and \ref{eq:formula_obj2}, respectively.

\begin{figure}[h!]
    \centering
    	\includegraphics[width=0.6\textwidth]{notation.pdf}
    \caption{Illustrating notation for a particular robot's trail plan.} \label{fig:notation}
\end{figure}

\subsubsection{The survival of a single robot along its followed trail}
Central to computing both $\mathbb{E}[S(\mathcal{P})]$ and $\mathbb{E}[R(\mathcal{P})]$ is the probability that a robot survives to reach a given node in its planned trail.
Let $S_n(\rho)$ for $0 \leq n \leq \lvert \rho \rvert$ be the Bernoulli random variable that is one if a robot following trail $\rho$ survives to visit the $n$th node in the trail and zero otherwise. 
For the event of survival, the robot must survive traversal of \emph{all} of the first $n$ arcs in its trail to visit node $\rho[n]$. More, the survival of a robot over each arc traversal attempt is an independent event. Therefore, the probability that a robot following trail $\rho$ successfully visits node $\rho[n]$ is the product of the survival probabilities of the first $n$ arc-hops in the trail:
\begin{equation}
	\pi[S_n(\rho) = 1] = \prod_{i=1}^n \omega(\rho[i-1], \rho[i]) 
	% \text{ for } 0 \leq n \leq \lvert \rho \rvert \\%\in \{1, ..., \lvert \rho \rvert\} 
	= 1 - \pi[S_n(\rho) = 0]. \label{eq:pi_S_n}
\end{equation} %The factorization owes to the independence of the arc-traversal$+$node-visit survival events.
The second equality follows because the complement of the event of survival is destruction.

\subsubsection{Expected number of robots that survive}
Now, we write a formula for the expected number of robots that survive the mission, $\mathbb{E}[S(\mathcal{P})]$. The event of survival of each robot is independent of the other robots.
Consequently, the number of robots that survive the mission, $S$, is the sum of the Bernoulli random variables indicating the survival of each robot over its entire planned trail:
\begin{equation}
	S(\mathcal{P}=\{\rho_1, ..., \rho_K\})=\sum_{k=1}^K S_{\lvert \rho_k \rvert}(\rho_k). \label{eq:R_sum}
\end{equation}
Thus, $S$ follows the Poisson-Binomial distribution \cite{tang2023poisson}.
%Specifically, the probability that $s$ robots survive the mission is:
%\begin{multline}
%	\pi[S(\mathcal{P})=s] = \sum_{\substack{\mathcal{R} \subseteq \{1, ..., K\}  \\ \lvert \mathcal{R} \rvert = s} } \,
%	\prod_{k \in \mathcal{R}} \pi[S_{\lvert \rho_k \rvert}(\rho_k) = 1]
%	\prod_{k^\prime \in \{1, ..., K\} \setminus \mathcal{R}}
%	 \pi[S_{\lvert \rho_{k^\prime} \rvert}(\rho_{k^\prime}) = 0]
%	 , \\ \text{ for } 0 \leq s \leq K.
%	\label{eq:R_pb}
%\end{multline}
%Eqn.~\ref{eq:R_pb} sums over all $\binom{K}{s}$ possible size-$s$ surviving subsets $\mathcal{R}$ of the $K$ robots. The first product is the probability that all of those robots in $\mathcal{R}$ indeed survive their closed trails. The second product is the probability that the other robots outside of $\mathcal{R}$ indeed get destroyed somewhere along their closed trails.
Seen from eqn.~\ref{eq:R_sum} and the linearity of the expectation operator, the expected number of robots that survive the mission is:
\begin{equation}
	\mathbb{E}[S(\mathcal{P}=\{\rho_1, ..., \rho_K\})]=\sum_{k=1}^K \mathbb{E}[S_{\lvert \rho_k \rvert}(\rho_k)] = \sum_{k=1}^K  \pi[S_{\lvert \rho_k \rvert}(\rho_k) = 1] \label{eq:formula_obj2}
\end{equation} with $\pi[S_{\lvert \rho_k \rvert}(\rho_k) = 1]$ given in eqn.~\ref{eq:pi_S_n}.

\subsubsection{Expected team reward}
Now, we write a formula for the expected team reward, $\mathbb{E}[R(\mathcal{P})]$. 
% Obviously, the outcome $S_n(\rho)=0$ implies $S_{n+i}(\rho)=0$ for $1 \leq i \leq \lvert \rho \rvert-n$ since, once a robot is destroyed at some node/arc along its trail, it cannot visit subsequent nodes in the planned trail.

First, we calculate the probability that a robot following a given trail $\rho$ does not visit a given node $v\in \mathcal{V}$.
Let the Bernoulli random variable $T_v(\rho)$ be one if a robot following trail $\rho$ visits node $v$ and zero if it does not.
If node $v$ is not in the planned trail $\rho$, $T_v(\rho)=0$ with certainty. 
Now, suppose node $v$ is in the planned trail $\rho$. Let $n^*$ be the index in the trail where the robot plans its first visit to node $v$:
\begin{equation}
n^*(\rho, v) = \min_{
	\substack{n \in \{0, ..., \lvert \rho \rvert\} \\ \rho[n] = v}
} n.
\end{equation}
Then, $T_v(\rho)=S_{n^*}(\rho)$ because the robot visits node $v$ if and only if it survives its first $n^*$ arc-hops to first land on node $v$ in the trail. 
So, the probability node $v$ is not visited by a robot following trail $\rho$ is:
\begin{equation}
	\pi[T_v(\rho) = 0] = 
	\begin{cases}
		1 & v\notin \rho\\
		 \pi [S_{n^*(\rho, v)}(\rho)=0 ] & v \in \rho
	\end{cases}
	 \label{eq:pi_T_v}
\end{equation}
with $\pi[S_{n^*}(\rho)=0]$ calculated using eqn.~\ref{eq:pi_S_n}.

Second, we calculate the probability that a given node $v\in\mathcal{V}$ is visited by one or more robots on a team following trail plans $\mathcal{P}=\{\rho_1, ..., \rho_K\}$. Only in this event is the reward offered by that node, $r(v)$, accumulated by the team.
Let $T_v(\mathcal{P})$ be the Bernoulli random variable that is one if one or more robots on the team with trail plans $\mathcal{P}$ visit node $v$ and zero otherwise (i.e.\ if and only if zero of the robots visit $v$).
The complement of the event $T_v(\mathcal{P})=1$ is that all $K$ robots do not visit node $v$ (independent events), so:
\begin{equation}
	\pi [T_v( \{\rho_1, ..., \rho_K\} ) = 1] = 
	1 - \prod_{k=1}^K \pi[T_v(\rho_k)=0].
	\label{eq:pi_T_v_all}
\end{equation} 
with $\pi[T_v(\rho) = 0]$ in eqn.~\ref{eq:pi_T_v}.

Finally, the total team reward collected by the robot-team following trail plan $\{\rho_1, ..., \rho_K\}$ is the sum of the rewards given to the team by each node:
\begin{equation}
R(\{\rho_1,...,\rho_K\}) = \sum_{v\in\mathcal{V}} r(v)  T_v(\{\rho_1, ..., \rho_K\}),
\end{equation} where the reward from node $v$, $r(v)$, is accumulated if and only if node $v$ is visited by one or more robots (i.e. iff $T_v(\mathcal{P})=1$).
The expected reward accumulated over the mission by a robot-team following trail plans $\{\rho_1, ..., \rho_K\}$ is:
\begin{equation}
	\mathbb{E}[R(\{\rho_1,...,\rho_K\})]= \sum_{v\in\mathcal{V}} r(v) \pi[T_v(\{\rho_1, ..., \rho_K\}) = 1] \label{eq:formula_obj1}
\end{equation}
with $ \pi[T_v(\{\rho_1, ..., \rho_K\}) = 1]$ given in eqn.~\ref{eq:pi_T_v_all}.

\subsubsection{Summary: computing the objective values associated with a robot-team trail plan}
Given a robot-team trail plan $\mathcal{P}:=\{\rho_1,...,\rho_K\}$, we can compute the expected team reward, $\mathbb{E}[R(\mathcal{P})]$, via eqn.~\ref{eq:formula_obj1} and the expected number of robots that survive the mission, $\mathbb{E}[S(\mathcal{P})]$, via eqn.~\ref{eq:formula_obj2}.


% harvest risk different from visit risk. once harvested, then no longer risk for other robots to visit that node.

\section{Bi-objective ant colony optimization to search for the Pareto-optimal robot-team trail plans}
To efficiently search for the Pareto-optimal set of robot-team trail plans defined by eqn.~\ref{eq:the_two_objs}, we employ bi-objective (BO) ant colony optimization (ACO) \cite{iredi2001bi}. ACO \cite{dorigo2006ant,bonabeau1999swarm,blum2005ant,simon2013evolutionary} is a meta-heuristic for combinatorial optimization problems that can be framed as a search for optimal paths or trails on a graph. As a swarm intelligence method \cite{bonabeau1999swarm}, 
ACO is inspired by the behavior of ants efficiently foraging for food via laying and following pheromone trails \cite{bonabeau2000inspiration}. See Box~\ref{box:ants}. 
Variants of the single-robot- and team-orienteering problem have been efficiently and effectively solved by ACO \cite{ke2008ants,chen2015multiobjective,verbeeck2017time,sohrabi2021acs,chen2022environment,montemanni2011enhanced} (as well as other [meta-]heuristics \cite{gavalas2014survey,dang2013effective,chao1996fast,butt1994heuristic}), but not the TSOP.

\begin{mybox}[label=box:ants, breakable]{Pheromone trail laying and following by foraging ants}
Advantageously \cite{deneubourg1983probabilistic}, a foraging ant colony can (well, for some ant species) collectively select 
(i) the shortest path from its nest to a food source \cite{goss1989self}
and
(ii) the highest quality food source among multiple options equidistant from its nest \cite{beckers1993modulation}. 
Largely, but not exclusively \cite{evison2008combined,czaczkes2015trail,robinson2005no}, this self-organizing of the colony is mediated by individual ants laying and following pheromone trails \cite{czaczkes2015trail}.

Pheromone is a relatively volatile chemical \cite{david2009trail} that an ant can both secrete and receive as an olfactory cue \cite{knaden2016sensory}. Thereby, pheromone allows for \emph{indirect} communication between ants in the colony. 
On its way back to the nest from a food source, an ant deposits pheromone on the ground in proportion to the quality of the food source \cite{beckers1993modulation}.
Shorter paths to higher quality food sources accumulate pheromone from ants more quickly.
Since ants are recruited to and follow pheromone \cite{beckers1993modulation,czaczkes2015trail}, these paths recruit more ants and get reinforced. Via this positive feedback mechanism, the colony can collectively select the shortest paths from the nest to the highest-quality food sources \cite{jackson2006communication,czaczkes2015trail,bonabeau1999swarm}.
The pheromone trails laid by the colony form a \emph{collective memory}, locally guiding ants to high-quality food sources and short paths to them \cite{jackson2006communication}.
Some species of ants can deposit multiple species of pheromone (from different glands) with e.g.\ different longevities \cite{czaczkes2015trail}, allowing for more complex indirect communication \cite{jackson2006communication,robinson2005no}.

As negative feedback mechanisms, 
(i) pheromone evaporates over time \cite{jackson2006communication,david2009trail,van2011temperature}, allowing the ant colony to ``forget'' trails to exhausted food sources,
and 
(ii) ants deposit less pheromone on trails (a) with already-high pheromone concentrations \cite{czaczkes2013ant} or (b) leading to food sources already occupied by their nestmates \cite{wendt2020negative}.

Ants select among pheromone trails with some degree of stochasticity \cite{deneubourg1990self}, which is beneficial for 
(i) continual exploration to find even shorter paths and even higher-quality food sources, 
(ii) exploiting multiple food sources in parallel, 
and 
(iii) plasticity in a dynamic environment \cite{deneubourg1983probabilistic,shiraishi2019diverse,deneubourg1986random,dussutour2009noise,edelstein1995trail}.


In summary, a colony of decentralized ants laying and following pheromone trails can exhibit complex, self-organizing behavior.
The pheromone trails serve as a form of collective memory and a means for indirect communication.
Loosely, ants can solve optimization problems through combining positive feedback (reinforcing good paths), negative feedback (pheromone evaporation), and randomness (exploring new paths).\cite{bonabeau1997self,bonabeau1999swarm,goss1989self,jackson2006communication,edelstein1995trail,watmough1995modelling}.
\end{mybox}

In bi-objective ACO \cite{iredi2001bi}, we simulate a heterogenous colony of artificial ants walking on the graph $G$ and, by loose analogy, foraging for food over many iterations. 
At each iteration, each worker ant stochastically constructs a robot-team trail plan $\{\rho_1, ..., \rho_K\}$ robot-by-robot, arc-by-arc, biased by 
(i) the amounts of two species of pheromone on the arcs that encode the colony's memory of trails that gave high reward and high survival
and 
(ii) two heuristics that score the greedy, \emph{a priori} appeal of each arc for constructing trails that give high reward and high survival.  
As a division of labor, each worker ant specializes by searching for team trail plans belonging to a different region of the Pareto-front.
At the end of each iteration, ants that found Pareto-optimal team trail plans over that iteration deposit reward and survival pheromone on the arcs involved, in proportion to the expected reward and survival, respectively, achieved by the plan.
An elitist ant \cite{dorigo1996ant} maintains a set of global (i.e, over all iterations) Pareto-optimal team trail plans and also deposits pheromone on arcs.
Finally, to prevent stagnation and promote continual exploration, a fraction of the pheromone evaporates each iteration. After many iterations, the ACO algorithm returns the [approximate\footnote{The ACO meta-heuristic is not guaranteed to find all of the Pareto-optimal solutions nor neglect to include a Pareto-dominated solution.}] Pareto-optimal set of robot-team trail plans maintained by the elitist ant. 


\subsection{The heterogenous artificial colony of ants, the pheromone, and the heuristics}
Our heterogenous artificial ant colony consists of (i) $N_{\text{ants}}$ worker ants and (ii) an elitist ant.
Worker ant $i\in\{1, ..., N_{\text{ants}}\}$ in the colony is assigned a parameter $\lambda_i := (i-1) / (N_{\text{ants}}-1)$ dictating its balance of the two objectives when searching for Pareto-optimal team trail plans.
A $\lambda$ closer to zero (one) implies the ant prioritizes maximizing the expected reward $\mathbb{E}[R]$ (robot survivals $\mathbb{E}[S]$). 
So, different ants seek team trail plans belonging to different regions of the Pareto front.
The elitist ant maintains the global-Pareto-optimal set of team trail plans.

% \subsection{Pheromone levels and heuristic scores on the arcs of the graph}
Each arc $(v, v^\prime)\in\mathcal{E}$ of the graph is associated with 
(i) amounts of two distinct species of pheromone, reward pheromone $\tau_R(v, v^\prime)$ and survival pheromone $\tau_S(v, v^\prime)$, and 
(ii) two heuristic scores $\eta_R(v, v^\prime)$ and $\eta_S(v, v^\prime)$.
Both $\tau_{R,S}(v, v^\prime)>0$ and $\eta_{R,S}(v, v^\prime)>0$ score the promise of arc $(v, v^\prime)$ for belonging to Pareto-optimal team trail plans that maximize $\mathbb{E}[R]$ and $\mathbb{E}[S]$, respectively, and guide worker ants' construction of robot-team trail plans.
The pheromone is (1) learned, reflecting the past collective experience/memory of the ant colony and (2) dynamic over iterations, due to deposition by the ants and evaporation.
By contrast, the heuristic is static and scores the \emph{a priori}, greedy/myopic appeal of each arc.
We incorporate the heuristics into ACO to accelerate its convergence.
%
%
%The ants lay and [probabilistically] follow two distinct species of artificial pheromone on the arcs of the graph $G$---one species associated with each objective. 
%
%The pheromone maps $\tau_R:\mathcal{E}\rightarrow \mathbb{R}_+$ and $\tau_S:\mathcal{E}\rightarrow \mathbb{R}_+$ give the amount of reward and survivability pheromone, respectively, on each arc of $G$.
%The value $\tau_R(v, v^\prime)$ ($\tau_S(v, v^\prime)$) scores the promise of arc $(v, v^\prime)$, learned from all past experience of the ant colony, for belonging to Pareto-optimal team trail plans that maximize the expected reward $\mathbb{E}[R]$ (number of robots that survive $\mathbb{E}[S]$).
%As opposed to being static, the pheromone maps $\tau_{R,S}$ change from iteration-to-iteration due to deposition by the ants and evaporation. 
% Together, the pheromone maps $\tau_{R,S}$ represent a [fading] memory of the artificial ant colony about the propensity of each arc to belong to Pareto-optimal robot-team trail plan and, vaguely, where along the Pareto-front the plan might li.e.
% vaguely, through laying and following \emph{two} species of pheromone, .

\subsection{Constructing robot-team trail plans}
Each iteration, every worker ant stochastically constructs a team trail plan $\mathcal{P}=\{\rho_1, ..., \rho_K\}$ by sequentially allocating trails to the robots while [conceptually] following the closed trail the ant designs for each robot. Computing the objectives achieved under each plan via eqns.~\ref{eq:formula_obj2} and \ref{eq:formula_obj1} gives the colony data $\{ (\mathcal{P}_i, \mathbb{E}[R(\mathcal{P}_i)], \mathbb{E}[S(\mathcal{P}_i)])\}_{i=1}^{N_{\text{ants}}}$. This data is used by the worker ants to deposit pheromone on the arcs and by the elitist ant to update the global Pareto-optimal set.
% It assigns trails to the robots sequentially, robot-by-robot. 
% i.e. fully constructs the closed trail for robot 1, $\rho_1$, then $\rho_2$ for robot 2, and so on, up to robot $K$.
% For each robot trail it constructs, the ant starts at the base node $v_b$, then, arc-hop by arc-hop, constructs the closed trail on the graph that the robot will follow. 

A worker ant constructs a robot trail by iteratively applying a stochastic, partial-trail extension rule until the closed trail is complete. Suppose an ant with objective-balancing parameter $\lambda$ is constructing the closed trail for robot $k$, $\rho_k$, and currently resides at node $v=\rho_k[i]$.
Namely, the ant has selected (i) the trails for the previous $k-1$ robots, $(\rho_1, ..., \rho_{k-1})$, and (ii) an incomplete/partial trail for robot $k$, $\tilde{\rho_k}=(v_b, \rho_k[1], ..., \rho_k[i]=v)$, giving its first $i$ arc-hops.
The ant extends the partial trail $\tilde{\rho_k}$ for robot $k$ by choosing the next node $\rho_k[i+1]$  
via a roulette-wheel selection of outgoing arcs of $v$ not yet traversed in $\tilde{\rho_k}$.
Specifically, the probability of next hopping to node $v^\prime$ across arc $(v, v^\prime)\in\mathcal{E}$ not yet traversed in the partial trail $\tilde{\rho_k}$ is:
 \begin{equation}
	\pi(v^\prime \mid \rho_1, ..., \rho_{k-1}, \tilde{\rho_k}) \propto 
		 \left[\tau_R(v, v^\prime) \eta_R(v, v^\prime; \rho_1, ..., \rho_{k-1},\tilde{\rho_k}) \right]^{1-\lambda} \left[ \tau_S(v, v^\prime) \eta_S(v, v^\prime) \right]^\lambda.
	 \label{eq:prob_x_y}
\end{equation}
The partial trail is more likely to be extended with $v^\prime=:\rho_k[i+1]$ if arc $(v, v^\prime)$ has more pheromone $\tau_{R, S}(v, v^\prime)$ and/or greedy heuristic appeal $\eta_{R, S}(v, v^\prime)$---with more or less emphasis on the reward or survivability pheromone/heuristic depending on the ant's $\lambda$.
Note, the probability of transitioning to a node $v^\prime$ is zero if $(v, v^\prime) \notin \mathcal{E}$ or if arc $(v, v^\prime) \in \mathcal{E}$ was already traversed in the partial trail $\tilde{\rho_k}$.
Iteratively extending the partial trail using eqn.~\ref{eq:prob_x_y}, the ant completes the trail for robot $k$ after it traverses the self-loop of the base node, $(v_b, v_b)$, signifying an end. Then, the ant begins trail construction for robot $k+1$ if $k<K$ or completes its team trail plan $\mathcal{P}$ if $k=K$. 


% While the pheromone maps $\tau_{R,S}$ encode the ants colony's past experiences and are dynamic over iterations, the static, heuristic, experience-independent maps $\eta_{R,S}(v, v^\prime)$ score the [greedy] appeal, in terms of maximizing $\mathbb{E}[R]$ and $\mathbb{E}[S]$, respectively, of hopping from $v$ to $v^\prime$. 

\paragraph{Heuristics.} 
For the survivability objective, we score the desirability of arc $(v, v^\prime)$ with the probability of the robot surviving traversal of that arc, $\eta_S(v, v^\prime):=\omega(v, v^\prime)$. This survival heuristic is myopic because it does not consider survivability of arcs later in the trail. 
For the reward objective, we greedily score the desirability of arc $(v, v^\prime)$ with the expected marginal reward the team receives by robot $k$ visiting node $v^\prime$ next, which is $r(v^\prime)$ if 
(i) none of the previous $k-1$ robots visit node $v^\prime$, 
(ii) node $v^\prime$ is not planned to be visited earlier in the trail of robot $k$, and
(iii) robot $k$ survives its hop to node $v^\prime$
and zero otherwise:
% Consequently, $\eta_R$ considers both the previous $k-1$ robots' trails and robot $k$'s partial trail $\tilde{\rho_{k}}$:
\begin{equation}
	\eta_R(v, v^\prime; \rho_1, ..., \rho_{k-1}, \tilde{\rho_k}) :=  
	 \pi[ T_{v^\prime}(\{\rho_1, ..., \rho_{k-1}\}) = 0)] \mathcal{I}[v^\prime \notin \tilde{\rho_k}] \omega(v, v^\prime) r(v^\prime ) \label{eq:eta_R}
\end{equation}
with $\mathcal{I}(\cdot)$ the indicator function. This reward heuristic is myopic because it does not account for rewards robot $k$ could collect further along the trail nor the trails of the remaining robots on the team. 
% {\color{red} handle negative rewards?}
Note, to prevent either heuristic from being exactly zero (resulting in \emph{never} selecting that arc), we add a small number $\epsilon$ to each heuristic.


\subsection{Pheromone update}
At the end of each iteration, we update the pheromone maps $\tau_R$ and $\tau_S$ to capture the experience of the ants in finding Pareto-optimal robot-team trail plans, better-guide the ants' trail-building in the next iteration, and prevent stagnation (premature convergence). 


The pheromone update rule is:
\begin{equation}
	\tau_{R, S}(v, v^\prime) \leftarrow (1-\rho) \tau_{R,S}(v, v^\prime)  + \Delta \tau_{R,S}(v, v^\prime) \text{ for } (v, v^\prime) \in \mathcal{E}, \label{eq:tau_update}
\end{equation}
with $\rho \in (0, 1)$ the evaporation rate (a hyperparameter) and $\Delta \tau_{R,S}(v, v^\prime)$ the amount of new pheromone deposited on arc $(v, v^\prime)$ by the ants.

Evaporation, accomplished by the first term in eqn.~\ref{eq:tau_update}, removes a fraction of the pheromone on every arc. This negative feedback mechanism prevents premature convergence to suboptimal trails and encourages continual exploration.

Deposition, accomplished by the second term in eqn.~\ref{eq:tau_update}, constitutes indirect communication to ants in future iterations about which arcs tend to belong to Pareto-optimal team trail plans and the value of the expected reward and robot survivals achieved under those plans.
First, the ants \emph{collaborate} by 
(i) among the worker ants, comparing the solutions constructed \emph{this} iteration to obtain the \emph{iteration}-Pareto-optimal set of plans
and 
(ii) worker ants sharing the solutions constructed this iteration with the elitist ant, who then updates the \emph{global}-Pareto-optimal set.
Second, the worker ants with an iteration-Pareto-optimal team trail plan execute their constructed plan while depositing pheromone on the arcs.
Third, the elitist ant executes all global-Pareto-optimal team trail plans while depositing pheromone on the arcs. Each ant deposits both reward and survival pheromone in proportion to the reward and survival objectives, respectively, achieved under the plan they are following. (The value of the objective is analogous with food quality). 
Specifically, amalgamating the iteration- and global-Pareto-optimal team trail plans into a multiset $\{(\mathcal{P}_p^*, \mathbb{E}[R(\mathcal{P}_p^*)], \mathbb{E}[R(\mathcal{P}_p^*)])\}_{p=1}^P$, arc $(v, v^\prime)$ receives pheromone:
%Now, each time arc $(v, v^\prime)$ is traversed in trails of a [iteration- or global-]Pareto-optimal plan $\mathcal{P}^*_p$, 
%this [worker- or elitist-] ant deposits pheromone of each species on it in proportion to the objective achieved with $\mathcal{P}^*_p$:
\begin{equation}
	 \Delta \tau_{R,S}(v, v^\prime) := 
	\frac{1}{P} \sum_{p=1}^{P} \mathbb{E}[R(\mathcal{P}^*_p), S(\mathcal{P}^*_p)] 
	\kappa_{v, v^\prime}(\mathcal{P}^*_p) 
%	% sum over trails in the plan
%	\sum_{k=1}^K 
%	% sum over arcs in the trail
%	\sum_{i=1}^{\lvert \rho_k^{(p)}\rvert}
%	% indicator
%	\mathcal{I} \left[ 
%		(v, v^\prime)=(\rho_k^{(p)}[i-1], \rho_k^{(p)}[i])
%	\right].
\end{equation}
where $\kappa_{v, v^\prime}(\mathcal{P}^*_p)$ counts the number of times arc $(v, v^\prime) \in \mathcal{E}$ appears in team trail plan $\mathcal{P}^*_p$.
%The sums are over, left-to-right, Pareto-optimal plans, robot trails in those plans, and arcs in those robot trails. 
%The latter two sums count the number of times the arc $(v, v^\prime)$ appears in $\mathcal{P}^*_p$.
By construction, arcs that receive the most reward (survival) pheromone frequently belong to trails in the Pareto-optimal team trail plans with high reward (survival).

We initialize the pheromone on all arcs with $\tau_{R,S}(v, v^\prime)=1$, to allow the heuristic to completely guide the first iteration. 
% TODO explain rho nottaion. gotta expand that. the rho_k^(p) not defined.

\subsection{Shared versus per-robot pheromone maps}
The reward and survival pheromone maps are shared by the ant colony as a form of collective memory of arcs that led to Pareto-optimal trails with high expected reward and survival. Through sharing the two pheromone maps, the ants in the colony, despite focusing on different regions of the Pareto-front, collaborate in finding Pareto-optimal robot-team trail plans.
More, seen in the trail-construction rule in eqn.~\ref{eq:prob_x_y} and the pheromone-laying rule in eqn.~\ref{eq:tau_update}, each ant uses the same pair of pheromone trails to construct the trail plan for all of the robots. (Still, through the heuristic $\eta_R$ in eqn.~\ref{eq:eta_R}, when an ant is sequentially allocating trails to the robots, it is aware of the trail plans $\rho_1, ..., \rho_{k-1}$ for the previously-planned robots when constructing the trail plan $\rho_k$ for robot $k$.)
A contrasting approach is to maintain a distinct, independent pair of pheromone maps for each robot on the team \cite{bell2004ant}. 
This would allow pheromone to explicitly promote sending the first robot to one region of the graph, the second robot to a different region, and so on. However, allocating a pair of pheromone trails to each robot would require more memory and seems to conflict with the inherent permutation-invariance in the homogenous team of robots. Though, order could be imposed onto the robots after their trails are assigned, based on features of their trails.


%Note, one may propose to 
%Instead, we experiment with an additional ACO variant where $K$ pheromone maps $\{\tau_{R}^{(k)}(v, v^\prime)\}_{k=1}^K$ are maintained and individualized to each robot on the team.
%When an ant constructs a trail for robot $k$, it is biased by the pheromone maps $\tau_{R, S}^{(k)}(v, v^\prime)$. More, an ant updates the pheromone map $\tau_{R, S}^{(k)}(v, v^\prime)$ using only robot $k$'s trail in the Pareto-optimal team trail plan. 
%Clashing with approach, the robot team is homogenous, implying permutation-invariance in the order in which we store the trails in the robot-team trail plan.
%Consequently, we promote symmetry-breaking during pheromone-laying by first sorting the robots in a trail plan by the lengths of their trails. The first (last) pheromone trail corresponds with the robot with the shortest (longest) trail. 
%(One could imagine more sophistical approaches for sorting the robots to promote symmetry breaking.) Intuitively, allocating one pair of pheromone trails to each robot instead of sharing a pair of pheromone maps among all robots could either
%(1) slow convergence, since feedback is now not shared among robots, or 
%(2) speed convergence, as now pheromone can explicitly promote e.g. the first-constructed robot trail to explore a different region of the graph than the second. 
%Note, under the robot-shared pheromone maps, still the sequential allocation of trails to the robots is aware of the trails of the previous robots through the heuristic $\eta_R$ in eqn.~\ref{eq:eta_R}.


% The elitist ant reinforces pheromone trails belonging to the global-Pareto-optimal set to encourage future ants to explore variations of them, while reinforcing the iteration-Pareto-optimal set helps promote exploration of variants of good trails.
  % TODO not quite, needs work

\subsection{Area indicator for Pareto-set quality}
From iteration-to-iteration, we measure the quality of the global [approximate] set of Pareto-optimal robot-team trail plans, tracked by the elitist ant, using the area in objective space enclosed between the origin and the [approximated] Pareto-front \cite{cao2015using,guerreiro2020hypervolume}. Formally, the quality $q$ of a Pareto set $\{\mathcal{P}^*_1, ...,\mathcal{P}^*_P\}$ is the area of the union of rectangles in 2D objective space:
\begin{equation}
	q(\{\mathcal{P}^*_1, ...,\mathcal{P}^*_{P}\}):=
	\Big \lvert 
		\bigcup_{p=1}^P \{ o \in \mathbb{R}^2 : o \geq 0 \wedge  o \leq (\mathbb{E}[R(\mathcal{P}^*_p)], \mathbb{E}[S(\mathcal{P}^*_p)]) \} 
	\Big \rvert \label{eq:q}	
\end{equation}
illustrated with the shaded yellow area in Fig.~\ref{fig:pareto_optimal}.
To disallow one objective to dominate the area indicator, we report the normalized area indicator by first normalizing the expected 
(i) survival by the number of robots and (ii) reward by the sum of rewards over nodes.

\section{Baseline search method: Simulated annealing}
As a competitive baseline against which to benchmark ACO, we implemented a bi-objective simulated annealing (SA) algorithm \cite{kirkpatrick1983optimization,simon2013evolutionary} to search for the Pareto-optimal set of robot-team trail plans.

\paragraph{Inspiration for simulated annealing.}
In metallurgy and materials science, annealing is the process of heating a crystalline material to a high temperature, then cooling it to improve its crystallinity (i.e., reduce defects) and, thereby, some property.
From a statistical thermodynamic perspective:
  at high temperatures, the system is highly disordered and commonly visits high-energy micro-states; as the system cools, it settles into exploring only lower-energy micro-states in a basin of the energy landscape and exhibits more order.
  For the purpose of reaching a highly-ordered, very low-energy state (among, likely, many local minima in the energy landscape) after cooling, the fluctuations at higher temperatures are important to overcome barriers in the energy landscape; hopefully, the system hops into the lowest-energy basins of the energy landscape, corresponding to highly crystalline materials, as the system cools.
  
Taking inspiration from annealing of crystalline materials, simulated annealing (SA) makes an analogy between the energy landscape of an atomistic system (which maps a micro-state to an energy) and an objective function (which maps a solution to a quality score) to search for a global-optimal solution to a multi-modal optimization problem.
  SA directly mimics a Metropolis Markov chain Monte Carlo simulation of the canonical statistical thermodynamic ensemble governing the micro-states of a crystalline material at fixed temperature, volume, and particle number, but decreases the temperature as the search proceeds (the cooling). The two main ingredients of an SA algorithm for optimization are: 
  (1) a mechanism to randomly perturb the current solution to propose moving to a new, neighboring solution (solution :: micro-state) and 
  (2) the cooling rate, i.e.\ how temperature should decrease over iterations.
  At higher temperatures, the SA search algorithm is more exploratory, then gradually transitions to exploitation as the temperature is lowered.  
    
\paragraph{Simulated annealing for a single-objective problem.}
First, we define a scalar objective function of the robot-team trail plan $\mathcal{P}$ that we wish to minimize with SA:
\begin{equation}
	E(\mathcal{P}):= -\left( w_r \mathbb{E}[R(\mathcal{P})] + (1-w_r )\mathbb{E}[S(\mathcal{P})]  \right) \label{eq:energy}
\end{equation} with $w_r \in [0, 1]$ the weight on the reward objective. 
While ACO maintains a population of robot-team trail plans, SA evolves a single plan.
We begin with an initial robot-team trail plan $\mathcal{P}_0$ then proceed through iterations.
At each iteration of SA, we 
(1) perturb the current solution $\mathcal{P}$ to obtain a new, neighboring solution $\mathcal{P}^\prime$ then accept or reject this perturbation based on the energy change and temperature then (2) reduce the temperature. 
Additionally, we keep a memory of the lowest-energy team trail plan so far. At the beginning of the iteration, we choose to randomly restart with the lowest-energy team trail plan with 1\% probability. After iterations are exhausted, we return the lowest-energy team trail plan.

\vspace{-\baselineskip}
\subparagraph{Sampling a neighbor team-trail plan.}
We sample a neighboring robot-team trail plan $\mathcal{P}^\prime$ by randomly perturbing each robot's trail plan in $\mathcal{P}$, through selecting to do one of the following at random: (i) insert a random new node into the trail, (ii) delete a random node in the trail, (iii) swap two random nodes in the trail, (iv) substitute a random node in the trail with another random node; (v) delete a random segment of the trail, or (vi) reverse the trail. Unlike working with paths in a complete graph, some coding effort was required to sample perturbations that lead to feasible trails. 

\vspace{-\baselineskip}
\subparagraph{Acceptance rules.}
Given the proposed new state $\mathcal{P}^\prime$, 
we compute the difference in energy, $\Delta E:= E(\mathcal{P^\prime})- E(\mathcal{P})$, from the current state $\mathcal{P}$.
If $\Delta E < 0$, where $\mathcal{P}^\prime$ is lower in energy than $\mathcal{P}$, we replace the old trail plan $\mathcal{P}$ with the new, better trail plan $\mathcal{P}^\prime$. 
If $\Delta E > 0$, where $\mathcal{P}^\prime$ is higher in energy than $\mathcal{P}$, we replace the old trail plan $\mathcal{P}$ with the new, worse trail plan $\mathcal{P}^\prime$ with probability $e^{-\Delta E / T}$ and reject it otherwise. The new plan is unlikely to be accepted if (i) it is much higher in energy (i.e., $\Delta E$ is large) or if (ii) the temperature $T$ is low. Hereby, the temperature controls exploration of the space of robot-team trail plans. 

\vspace{-\baselineskip}
\subparagraph{Cooling schedule.}
We adopt an exponential cooling schedule, where for each iteration, $T \leftarrow \alpha T$ with $\alpha=0.95$. The initial temperature is $T_0=0.2$. 

\vspace{-\baselineskip}
\subparagraph{Standardization.}
Different problems involve different numbers of robots and have different distributions of arc-survival probabilities and node-rewards.
To (1) properly balance the two objectives in the scalarized objective function in eqn.~\ref{eq:energy} and (2) make temperature transferrable across problems, we first standardize the expected rewards by the sum of rewards over all nodes and the expected survival by the number of robots.

\paragraph{Bi-objective simulated annealing.}
To handle our bi-objective problem with SA, we run single-objective SA on the scalarized objective function in eqn.~\ref{eq:energy} while, many times over the course of the search, (1) incrementally increasing the weight $w_r$ then (2) resetting the temperature to the initial temperature $T_0$. 
We begin with the reward weight $w_r=0$ and the robot-team trail plan where all robots stay at the base node for maximal survival.
Then, with a set budget of $i^2$ total iterations, we run through a sequence of $i$ reward weights $w_r$ equally-spaced on $[0, 1]$ and, for each, run a single-objective SA for $i$ iterations. The SA search for each scalarized objective function is initiated with the best team-trail plan according to the most recent scalarized objective function. This way, we can start with quite a low temperature, since not as much exploration is needed, compared to restarting from scratch for each scalarized objective function. 
The approximate Pareto-optimal set returned by bi-objective SA is the Pareto-optimal subset of the $i$ robot-team trail plans found by each optimization of a scalarized objective function.

\paragraph{Comparing bi-objective ACO and SA.}
ACO maintains a population of $N_{\rm ants}$ robot-team trail plans each iteration, while SA maintains just one.
For SA, disadvantageously, one must set the number of iterations \emph{a priori}; incrementally adding more iterations is not effective since we have already scanned weights on the objective. By contrast, adding more iterations to ACO is quite natural.
Thus, when comparing ACO and SA in our benchmarks: (1) we divide the total number of SA iterations by $N_{\rm ants}$ to give each the same number of robot-team trail-plan evaluations for comparison and (2) e.g. for 1000 and 10\,000 iterations, we run two separate SAs from scratch and thus plot them as points, since tracking progress is not natural. 

\section{Results}
We now implement bi-objective ant colony optimization (ACO) to search for Pareto-optimal robot-team trail plans for three instances of the bi-objective team orienteering in a hazardous environment (BOTOHE) problem.

While the first example is a random, two-community graph, the second and third examples are based on real environments, namely an art museum and a power plant.

For each case study, we visualize in Figs.~\ref{fig:two_community}-\ref{fig:power_plant}:
\begin{enumerate}[noitemsep, label=(\Alph*)]
\item the problem setup, i.e.\ the graph, node--to--reward map, arc--to--survival-probability map, and robot team initialization;
\item the search progress of bi-objective ACO---guided by (i) static heuristics that greedily score the appeal of traversing a given arc and (ii) the dynamic pheromone maps that encapsulate the memory of the ant colony over previous iterations---in terms of the quality of the Pareto-front, measured by the area indicator, over iterations (average and standard deviation over four runs);
\item the final Pareto-front of ACO, along with a sample of three robot-team trail plans to intuit the reward-survival tradeoff; and
\item the learned reward and survival pheromone trails at the end of ACO.
\end{enumerate}
More, we compare the search progress of ACO with benchmarks in panel (B): 
\begin{itemize}[noitemsep]
\item ACO with heuristics ablated, i.e.\ where $\eta_{R, S}(v, v^\prime):=1$ for all arcs $(v, v^\prime)\in\mathcal{E}$, to quantify the contribution of the heuristics for guiding the search;
\item ACO with pheromone ablated, i.e.\ where statically $\tau_{R, S}:=1$ for all arcs, to quantify the contribution of the pheromone for guiding the search;
\item a [naive] random search where statically $\eta_{R, S}=\tau_{R,S}:=1$ ;
\item a completely different and competitive approach, bi-objective simulated annealing (SA).
\end{itemize}

Our Julia code to reproduce our results is available at \url{github.com/SimonEnsemble/BO_ACO_TOHE}.

\paragraph{ACO settings.} Our simulated ant colony comprises $N_{\rm ants}=100$ artificial ants. We conduct 10\,000 to 100\,000 iterations depending on the problem.
We set the pheromone evaporation rate at $\rho=0.1$. To handle different numbers of robots and ranges of rewards for different problems, we initialize the pheromone maps with, for each arc $(v, v^\prime)$, $\tau_s(v, v^\prime) := K$ and $\tau_r(v, v^\prime) := \sum_{v \in \mathcal{V}} r(v)$.


  
\subsection{Synthetic graph with two communities}
Fig.~\ref{fig:two_community} shows the results for a synthetic, random graph containing two communities.

\paragraph{Setup.}
Fig.~\ref{fig:two_community}A shows the problem setup.
We sampled the two-community graph from a stochastic block model. Community \#1 was assigned 8 nodes; community \#2: 12 nodes.
The probability of joining (both ways) a pair of nodes both in community \#1 was 0.3; both in community \#2: 0.4; and, for two nodes in distinct communities, much lower: 0.025.
At the base node, $K=2$ robots begin.
We assigned a reward to each node by sampling from a distribution based on community membership (both Gaussian and truncated below zero; $\mu_1=5$, $\sigma_1=2$; $\mu_2=1$, $\sigma_2=1/2$).
We sampled arc survival probabilities from a distribution regardless of community membership (Gaussian with $\mu=0.9$, $\sigma=0.3$, truncated below zero and above one).


\paragraph{Search progress.}
Fig.~\ref{fig:two_community}B shows the search progress of ACO in terms of the normalized area indicator over iterations. After 10\,000 iterations, the area indicator for ACO plateaus, suggesting convergence. By comparison, ACO without the heuristics and ACO without pheromone trails perform quite poorly, but better than random search. 
More, ACO outperforms the competitive SA baseline over all four total iterations dedicated to SA.
Reflecting the competitiveness of SA, at 1\,000 and 10\,000 iterations, the SA baseline outperforms ACO without the heuristic and ACO without the pheromone.  
We conclude both the heuristic and pheromone are important in ACO for this problem.

\paragraph{Pareto-front and sample of Pareto-optimal robot-team trail plans.}
Fig.~\ref{fig:two_community}C shows the Pareto-front at the end of an ACO run. The yellow highlighted region represents the [albeit, un-normalized] area indicator that scores the quality of the Pareto-front. Three Pareto-optimal robot-team trail plans are highlighted, running down the Pareto front: high expected survival with low expected reward; medium survival and reward; and high reward with low survival. In the high survival, low reward plan, the first robot explores one node of community \#1 then takes a safe cycle through community \#2, while the other robot visits only three nodes across safe arcs.
In the medium survival and reward plan, we see more coverage, some division of labor among the two robots, yet also node visit redundancy. The first robot visits all of community \#1 while avoiding the most dangerous arc within it. 
The second robot first visits many nodes in community \#2, avoiding the most dangerous arcs, then visits some of community \# to reap the rewards therein in the case that the first robot fails, giving robustness.

\paragraph{Pheromone trails.}
Finally, Fig.~\ref{fig:two_community}D shows the learned pheromone trails after a run of ACO after 10\,000 iterations.
The reward and survival pheromone maps appear similar; the Pearson correlation coefficient between the reward and survival pheromone is 0.95. 
The reward pheromone exhibits a small correlation (coefficient: 0.2) with the reward on the destination node, and the survival pheromone exhibits a small correlation (coefficient: 0.45) with the arc survival probability. The lack of a strong correlation evidences that the pheromone contains information beyond the greedy heuristics that look at these.
Finally, note the asymmetry present, where sometimes arc $(v, v^\prime)$ has high pheromone but arc $(v^\prime, v)$ has low pheromone. This shows that the pheromone can promote the robot to take a counter-clockwise cycle over a clock-wise one.

\paragraph{ACO runtime.}

\begin{figure}[h!]
    \centering
    	\includegraphics[width=\textwidth]{block_model_results.pdf}
    \caption{
    BOTOHE problem on a random, two-community graph. 
    (a) Problem setup: the graph, node--to--reward map, arc--to--survival-probability map, and robot-team start.
    (b) Search progress of various algorithms. (Mean, standard deviation over four runs.)
    (c) Pareto front of robot-team trail plans at the end of a run of ACO and a sample of three plans.
    (d) Pheromone trails at the end of a run of ACO.
    } \label{fig:two_community}
\end{figure}

\subsection{Art Museum}

\paragraph{Setup.}
Fig.~\ref{fig:art_museum}A shows the problem setup, where a team of $K=3$ mobile robots are assigned an information-gathering mission in the San Diego Museum of Art. 
% We selected this art museum for (i) its rich connectivity between galleries, giving an interesting example, and (ii) its small size, allowing us to visualize, interpret, and intuit the solution to a BOTOHE problem on it.

We spatially model the art museum as a directed graph $G=(\mathcal{V}, \mathcal{E})$.
The set of 27 nodes $\mathcal{V}$ represents the 23 art galleries (rooms), the outside entrance to the building (base node $v_b$), the main entrance rooms on the first and second floors, and the stairway.
The set of arcs $\mathcal{E}$ represents direct passages/doorways between the rooms. 

Suppose traversing the art museum is hazardous for the robots, owing to
(i) adversarial security guards that (a) seek to prevent the robots from imaging the art and (b) possess the ability attack and/or capture the robots, and
(ii) obstacles that the robots could (a) crash into or (b) become entangled in.
To model risks of destruction or capture, we assign survival probabilities $\omega(i,j)$ for the arc(s)
(i) traversing the staircase of 0.8,
(ii) inside and in/out of the main entrance room of 0.9,
(iii) on the right side of the first floor of 0.97,
(iv) on the left side of the first floor of 0.95,
and
(v) on the second floor of 0.9.
So, the stairway is the most dangerous arc.

Further suppose, when a robot visits an art gallery in the museum, it images the art there and transmits this image back to the command center, providing utility to the command center. 
The utility of each image to the command center is scored by
% Note, (i) even if the robot that took this image is later destroyed or captured, the image irrevocably provides utility to the command center, and (ii) multiple visits by robot(s) to a gallery do not provide further marginal reward because the command center already has an image from the first robot visit.
the node reward map $r$ assigning rewards of
(i) 2/3 for large galleries,
(ii) 1/3 for medium-sized galleries,
(iii) 1 for small galleries that we suppose contain the most valuable art, and
(iv) 1/10 for five galleries that are in corners of the museum or behind the stairway.

The two objectives of the command center are to plan the trails of the robots in the art museum to maximize the (1) expected reward, via robots visiting art galleries, imaging them, then transmitting the images back to the command center, and (2) expected number of robots that return from the mission. 

\paragraph{Search progress.}

\paragraph{Pareto-front and sample of Pareto-optimal robot-team trail plans.}

Fig.~\ref{fig:pareto_front} shows the [approximate] Pareto-front of 167 solutions found by BO-ACO. 
Walking down the Pareto-front trades larger expected team reward for a smaller expected number of robots that return from the mission. The key motivation for treating the \emph{bi-objective} TOHE problem is to present this Pareto-front to a human decision-maker who ultimately chooses a robot-team trail plan that balances reward with robot survival.

Finally, Fig.~\ref{fig:pareto_front} shows four Pareto-optimal trail plans belonging to different regions of the Pareto-front. 
As we move down the front, plans offer larger expected reward but lower robot survival.
In the first plan, only a single robot enters the museum to image galleries offering the highest rewards on the left side of the first floor---avoiding the more dangerous right side of the second floor and the very dangerous staircase to the second floor. 
In the second plan, two robots are utilized: the first plans to cover much of the left side of the first floor, while the second plans to cover much of the right side of the first floor.
In the third plan, also utilizing two robots, the first robot plans to traverse most of the first floor and the second floor later. The second robot traverses much of the first floor. This redundancy in first-floor coverage builds robustness into the plans: even if the first robot gets destroyed early on, the second robot can still image most of the galleries on the first floor.
Finally, the fourth plan uses all three robots with much redundancy to achieve high expected reward. Still, the robots avoid taking the risk to enter the bottom left corner of the first floor, whose galleries offer only a small reward.

\paragraph{Pheromone trails.}
At the end of the ACO algorithm, Fig.~\ref{fig:pheromone} visualizes the pheromone maps $\tau_{R}$ and $\tau_S$.
 Note the clock-wise vs.\ counter-close-wise preferences in some cycles of high pheromone to pursue high-reward nodes earlier in the trails to maximize expected reward.

\paragraph{ACO runtime.}

\begin{figure}[h!]
    \centering
    	\includegraphics[width=\textwidth]{art_museum_results.pdf}
    \caption{
    BOTOHE problem in the San Diego Art Museum. 
    (a) Problem setup: the graph, node--to--reward map, arc--to--survival-probability map, and robot-team start.
    (b) Search progress of various algorithms. (Mean, standard deviation over four runs.)
    (c) Pareto front of robot-team trail plans at the end of a run of ACO and a sample of three plans.
    (d) Pheromone trails at the end of a run of ACO.
    } \label{fig:art_museum}
\end{figure}


\subsection{Nuclear Power Plant}
\paragraph{Setup.}

\paragraph{Search progress.}

\paragraph{Pareto-front and sample of Pareto-optimal robot-team trail plans.}

\paragraph{Pheromone trails.}

\paragraph{ACO runtime.}


\begin{figure}[h!]
    \centering
    	\includegraphics[width=\textwidth]{power_plant_results.pdf}
    \caption{
    BOTOHE problem in the Satsop Nuclear Power Plant. 
    (a) Problem setup: the graph, node--to--reward map, arc--to--survival-probability map, and robot-team start.
    (b) Search progress of various algorithms. (Mean, standard deviation over four runs.)
    (c) Pareto front of robot-team trail plans at the end of a run of ACO and a sample of three plans.
    (d) Pheromone trails at the end of a run of ACO.
    } \label{fig:power_plant}
\end{figure}

% As a hazardous environment containing locations that offer rewards to a robot team, we use the graph model of the Satsop Nuclear Power Plant in Elma, Washington from the Defense Advanced Research Projects Agency (DARPA) Subterranean (SubT) robotics challenge \cite{chung2023into} (Alpha course of the Urban Circuit Network \cite{github_darpa_subt}).





 
 




\section{Discussion}
Many applications for mobile robot teams---ranging from information-gathering, resource-delivery, detecting and mapping CBRN (chemical, biological, radiological, and nuclear) hazards, forest fire fighting, resource delivery, to protection of wildlife from poachers---involve traversing environments with hazards e.g., corrosive chemicals, adversaries with attack capabilities, obstacles, rough terrain/seas, or high winds. 
For each application, the robots must coordinate their trails for the team-level objective in a risk-aware manner: select the subset of locations to visit, assign the safest trails to the robots, balance reward and risk to the robots, and build redundancy into the plans to make the objective robust to the failure of robots on the team. 

Heavily inspired by the Team Surviving Orienteers Problem \cite{jorgensen2018team,jorgensen2017matroid,jorgensen2024matroid}, we posed the bi-objective team orienteering in a hazardous environment (BOTHE) problem. 
By finding the Pareto-optimal set of robot-team trail plans, we can present them to a human decision-maker who ultimately chooses the robot trail plans for the mission according to how he or she values the expected reward collected by the robot team compared with the expected number of robots that survive the dangerous mission.

We employed bi-objective ant colony optimization to search for the Pareto-optimal team trail plans. Despite lacking theoretical guarantees to find the Pareto-optimal set, ACO was effective and can be readily adapted to handle extensions to the TOHE problem.

\paragraph{Future work.}
%Much future work remains for path-planning of robots in adversarial/hazardous environments.
% harvesting the reward is dangerous. not visiting. 
For the BOTOHE we have posed, we wish to 
(i) devise an ACO algorithm to handle the online setting, where the robots adapt their planned trails during the mission, in response to observed failures of robots; 
(ii) devise local search algorithms to improve the robot trails the ants found after each iteration, accelerating the convergence of ACO \cite{dorigo2006ant}, 
(iii) devise and benchmark non-sequential ACO schemes to concurrently extend the trail plans of the robots \cite{ke2008ants}, and 
(iv) employ multi-colony bi-objective ant optimization \cite{iredi2001bi}.

Interesting and practical extensions of robot-team orienteering in adversarial/hazardous environments abstracted a graphs include treating (some of these ideas from Ref.~\cite{jorgensen2018team}): 
(i) a heterogenous team of robots with different (a) capabilities to harvest rewards from nodes and (b) survival probabilities for each arc traversal owing to e.g.\ stealth;
(ii) more complicated reward structures, e.g., time-dependent, stochastic, non-additive (correlated \cite{yu2014correlated}), multi-category, or multi-visit rewards;
(iii) fuel/battery constraints of the robots via nodes representing refueling, recharging, or battery-switching stations \cite{asghar2023risk,khuller2011fill,liao2016electric,yu2019coverage}; 
(iv) constraints on the rewards a robot can harvest e.g.\ for resource delivery applications where each robot holds limited cargo capacity \cite{coelho2014thirty};
(v) non-binary surviving states of the robots due to various levels of damage;
(vi) non-independent events of robots surviving arc-traversals;
(vii) risk metrics different from the expected value \cite{majumdar2020should}.
% TODO: cite green vehicle routing

Another interesting direction is to learn/modify the survival probabilities associated with the edges of the graph from data over repeated missions (an inverse problem \cite{burton1992instance}). 
Specifically, suppose we are uncertain about the survival probability $\omega(i, j)$ of each arc $(i,j)$. Within a Bayesian inference framework, we may impose a prior distribution on each $\omega(i,j)$. Then, when a robot survives or gets destroyed during a mission, we update the prior distribution with this feedback and the trail the robot took. 
The trail-planning of the robots over sequential missions may then balance (a) exploitation to harvest the most reward and take what appear to be, under uncertainty, the safest trails and (b) exploration to find even safer trails.

\section*{Acknowledgements} CMS acknowledges the Office of Naval Research Summer Faculty Research Program.

\bibliographystyle{unsrt}
\bibliography{refs}


\end{document}  
